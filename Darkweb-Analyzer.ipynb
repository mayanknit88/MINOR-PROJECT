{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit0413/Darkweb-Analyzer-/blob/main/Darkweb-Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1st Level Classification**"
      ],
      "metadata": {
        "id": "lj0PyTaPUzVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtyYQyOOVS6o",
        "outputId": "04f99fab-b181-4e92-ec33-ce7ed1556fbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Z6nuwrvzLN",
        "outputId": "2fb7d569-21b0-417d-fe31-7645c8ddd083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n"
          ]
        }
      ],
      "source": [
        "pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scapy"
      ],
      "metadata": {
        "id": "Hbo0GRtQwCpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a65c30-752a-4e34-fd3f-7d1f0b696e7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scapy\n",
            "  Downloading scapy-2.5.0.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: scapy\n",
            "  Building wheel for scapy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scapy: filename=scapy-2.5.0-py2.py3-none-any.whl size=1444328 sha256=9685ca89573958596eee2e4300fed598889a18c1a335517870dd2d2aae555465\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/b7/03/8344d8cf6695624746311bc0d389e9d05535ca83c35f90241d\n",
            "Successfully built scapy\n",
            "Installing collected packages: scapy\n",
            "Successfully installed scapy-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Load the labeled dataset\n",
        "df_labeled = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv/darknet-normal.csv')\n",
        "\n",
        "df_labeled.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "df_labeled.fillna(0, inplace=True)  # or\n",
        "\n",
        "# Define the features to keep, based on the extract_features function\n",
        "features_to_keep = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Packet Length Min', 'Packet Length Mean', 'Fwd IAT Total',\n",
        "    'Flow IAT Min', 'Flow IAT Max', 'Fwd IAT Mean', 'Flow Packets/s',\n",
        "    'Flow Bytes/s', 'Idle Min', 'Idle Max', 'Idle Mean',\n",
        "    'Idle Std', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'ACK Flag Count'\n",
        "]\n",
        "\n",
        "# Keep only the relevant features\n",
        "df_relevant_features = df_labeled[features_to_keep + ['Label']]\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df_relevant_features.drop('Label', axis=1)\n",
        "\n",
        "y = df_relevant_features['Label'].map({'Normal': 'normal', 'FreeNet': 'darknet', 'I2P': 'darknet', 'Tor': 'darknet', 'ZeroNet': 'darknet'})\n",
        "\n",
        "df_labeled.fillna(0, inplace=True)\n",
        "\n",
        "# Split into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply one-hot encoding only to the train dataset to avoid memory issues\n",
        "X_train = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Align X_train and X_test to ensure they have the same columns\n",
        "X_train, X_test = X_train.align(X_test, join='inner', axis=1)\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "Xgb_classify = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=42)\n",
        "\n",
        "# Encode the labels with LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Train the classifier with the encoded binary labels\n",
        "Xgb_classify.fit(X_train,y_train_encoded)\n",
        "\n",
        "# Make predictions with the encoded labels\n",
        "encoded_predictions = Xgb_classify.predict(X_test)\n",
        "\n",
        "# Decode the predictions back to original labels\n",
        "predictions = label_encoder.inverse_transform(encoded_predictions)\n",
        "accuracy = accuracy_score(y_test_encoded, encoded_predictions)\n",
        "precision = precision_score(y_test_encoded, encoded_predictions, pos_label=label_encoder.transform(['darknet'])[0])\n",
        "recall = recall_score(y_test_encoded, encoded_predictions, pos_label=label_encoder.transform(['darknet'])[0])\n",
        "f1 = f1_score(y_test_encoded, encoded_predictions, pos_label=label_encoder.transform(['darknet'])[0])\n",
        "\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "WauJC2gfzwbL",
        "outputId": "f88b9e27-3fd1-4f9a-9629-3a2a7652c85b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9944990484135882\n",
            "Precision: 0.9959913326110509\n",
            "Recall: 0.9926037898828484\n",
            "F1 Score: 0.9942946759321851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the model\n",
        "Xgb_classify.save_model('xgb_model.json')"
      ],
      "metadata": {
        "id": "EswxTM2fiJtL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "loaded_model = xgb.XGBClassifier()\n",
        "loaded_model.load_model('xgb_model.json')"
      ],
      "metadata": {
        "id": "g0f8X2n3izct"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scapy.all import rdpcap, IP, TCP\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def extract_features(pcap_file):\n",
        "    packets = rdpcap(pcap_file)\n",
        "    features = {\n",
        "        'Flow Duration': 0.0,\n",
        "        'Total Fwd Packet': 0,\n",
        "        'Total Bwd packets': 0,\n",
        "        'Packet Length Min': np.inf,\n",
        "        'Packet Length Mean': 0.0,\n",
        "        'Fwd IAT Total': 0.0,\n",
        "        'Flow IAT Min': np.inf,\n",
        "        'Flow IAT Max': 0.0,\n",
        "        'Fwd IAT Mean': 0.0,\n",
        "        'Flow Packets/s': 0.0,\n",
        "        'Flow Bytes/s': 0.0,\n",
        "        'Idle Min': np.inf,\n",
        "        'Idle Max': 0.0,\n",
        "        'Idle Mean': 0.0,\n",
        "        'Idle Std': 0.0,\n",
        "        'FWD Init Win Bytes': 0,\n",
        "        'Bwd Init Win Bytes': 0,\n",
        "        'ACK Flag Count': 0\n",
        "    }\n",
        "\n",
        "    if not packets:\n",
        "        return pd.DataFrame(features, index=[0])\n",
        "\n",
        "    start_times = []\n",
        "    packet_lengths = []\n",
        "    iats = []\n",
        "    total_bytes = 0\n",
        "\n",
        "    for packet in packets:\n",
        "        if IP in packet and TCP in packet:\n",
        "            packet_length = len(packet)\n",
        "            packet_lengths.append(packet_length)\n",
        "            total_bytes += packet_length\n",
        "\n",
        "            if 'S' in packet[TCP].flags:\n",
        "                if features['FWD Init Win Bytes'] == 0:\n",
        "                    features['FWD Init Win Bytes'] = packet[TCP].window\n",
        "                else:\n",
        "                    features['Bwd Init Win Bytes'] = packet[TCP].window\n",
        "\n",
        "            if 'A' in packet[TCP].flags:\n",
        "                features['ACK Flag Count'] += 1\n",
        "\n",
        "            start_times.append(float(packet.time))\n",
        "\n",
        "            if len(start_times) > 1:\n",
        "                iat = start_times[-1] - start_times[-2]\n",
        "                iats.append(iat)\n",
        "\n",
        "    features['Flow Duration'] = max(start_times) - min(start_times)\n",
        "    features['Total Fwd Packet'] = len([p for p in packets if IP in p and p[IP].src < p[IP].dst])\n",
        "    features['Total Bwd packets'] = len([p for p in packets if IP in p and p[IP].src > p[IP].dst])\n",
        "    features['Packet Length Min'] = min(packet_lengths)\n",
        "    features['Packet Length Mean'] = np.mean(packet_lengths) if packet_lengths else 0\n",
        "    features['Fwd IAT Total'] = sum(iats)\n",
        "    features['Flow IAT Min'] = min(iats) if iats else 0\n",
        "    features['Flow IAT Max'] = max(iats) if iats else 0\n",
        "    features['Flow IAT Min'] = np.mean(iats) if iats else 0\n",
        "    features['Flow Packets/s'] = len(packets) / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "    features['Flow Bytes/s'] = total_bytes / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "\n",
        "    # Handle potential NaNs and infs\n",
        "    for key, value in features.items():\n",
        "        if isinstance(value, float) and (np.isinf(value) or np.isnan(value)):\n",
        "            features[key] = 0\n",
        "\n",
        "    df_features = pd.DataFrame([features])\n",
        "\n",
        "# Handle potential NaNs and infs again before returning\n",
        "    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_features.fillna(0, inplace=True)\n",
        "\n",
        "    return df_features\n"
      ],
      "metadata": {
        "id": "IOyaNRLmpDMj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "def classify_traffic(df_features, model_path):\n",
        "    # Load the trained model\n",
        "    xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "    # Load the model\n",
        "    xgb_model.load_model(model_path)\n",
        "\n",
        "    # Ensure that the model has been fitted before making predictions\n",
        "    if not xgb_model.get_booster().attr(\"n_features\"):\n",
        "        raise ValueError(\"Model needs to be fitted before making predictions\")\n",
        "\n",
        "    # Predict the traffic class\n",
        "    predictions = xgb_model.predict(df_features)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "13BCBos7s1Vz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "model_path = '/content/xgb_model.json'  # Make sure this is the correct path to your model file\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/zeronet-p2p_00001_20200421125502.pcap'\n",
        "\n",
        "# Loading the trained XGBoost model\n",
        "xgb_classifier = xgb.XGBClassifier()\n",
        "xgb_classifier.load_model(model_path)\n",
        "\n",
        "# Extract features from the pcap file\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Output the predicted class\n",
        "print(\"Predicted Class:\", predictions[0])\n",
        "print(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "yjaOYMf-yUa2",
        "outputId": "75cb52f0-c814-489b-a7ff-6ff0e674e440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: darknet\n",
            "['darknet' 'normal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(df_features, pd.Series):\n",
        "    df_features = df_features.to_frame().transpose()\n",
        "\n",
        "predictions = xgb_classifier.predict(df_features)"
      ],
      "metadata": {
        "id": "Y8FgKgdl_Lqj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if predictions[0] == 0:\n",
        "    print(\"Darknet\")\n",
        "else:\n",
        "    print(\"Normal\")"
      ],
      "metadata": {
        "id": "F1fFX3Zx_QXj",
        "outputId": "408e486a-d741-494b-b8e6-4f068e7f7506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Darknet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2nd Level Classification**"
      ],
      "metadata": {
        "id": "__hRaVqSUk3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scapy"
      ],
      "metadata": {
        "id": "ZNreMEZr8MSP",
        "outputId": "d6a21b8b-9336-4e80-a987-03ddd89674d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scapy in /usr/local/lib/python3.10/dist-packages (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming numerical features are already identified in features_to_keep\n",
        "numerical_features = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Flow Bytes/s', 'Flow Packets/s', 'Fwd IAT Mean',\n",
        "    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "    'Packet Length Min', 'Packet Length Mean',\n",
        "    'ACK Flag Count', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Save the scaler for future use\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.save')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming numerical features are already identified in features_to_keep\n",
        "numerical_features = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Flow Bytes/s', 'Flow Packets/s', 'Fwd IAT Mean',\n",
        "    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "    'Packet Length Min', 'Packet Length Mean',\n",
        "    'ACK Flag Count', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
        "]\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Save the scaler for future use\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.save')"
      ],
      "metadata": {
        "id": "YsZ8NGsK84qW",
        "outputId": "a8e0c4a9-1f05-4cc5-da6a-bc207b96ca1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.save']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "df_labeled = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv/darknet-type.csv')\n",
        "\n",
        "# Convert non-numeric columns to numeric\n",
        "label_encoders = {}\n",
        "for column in df_labeled.columns:\n",
        "    if df_labeled[column].dtype == 'object' and column != 'Label':\n",
        "        le = LabelEncoder()\n",
        "        df_labeled[column] = le.fit_transform(df_labeled[column].astype(str))\n",
        "        label_encoders[column] = le\n",
        "\n",
        "# Replace inf/-inf with NaN and impute missing values\n",
        "df_labeled.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df_labeled.iloc[:, df_labeled.columns != 'Label'] = imputer.fit_transform(df_labeled.iloc[:, df_labeled.columns != 'Label'])\n",
        "\n",
        "# Define and extract features\n",
        "features_to_keep = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Flow Bytes/s', 'Flow Packets/s', 'Fwd IAT Mean',\n",
        "    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "    'Packet Length Min', 'Packet Length Mean', 'ACK Flag Count',\n",
        "    'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Idle Mean',\n",
        "    'Idle Std', 'Idle Max', 'Idle Min',\n",
        "\n",
        " ]  # List of features as provided\n",
        "X = df_labeled[features_to_keep]\n",
        "y = df_labeled['Label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the hyperparameter space for Random Search\n",
        "param_dist = {\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "    'reg_lambda': [0.01, 0.1, 1, 10],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1]\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', use_label_encoder=False, num_class=len(np.unique(y_train_encoded)), seed=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,  # Number of parameter settings that are sampled\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the scaled training data\n",
        "random_search.fit(X_train_scaled, y_train_encoded)\n",
        "\n",
        "# Select the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Prediction using the best model\n",
        "predictions_encoded = best_model.predict(X_test_scaled)\n",
        "predictions = label_encoder.inverse_transform(predictions_encoded)\n",
        "\n",
        "# Performance metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "f1 = f1_score(y_test_encoded, predictions_encoded, average='weighted')\n",
        "\n",
        "print(f\"Enhanced Accuracy: {accuracy}\")\n",
        "print(f\"Enhanced F1 Score: {f1}\")\n",
        "\n",
        "# Save the scaler and the best model\n",
        "joblib.dump(scaler, 'scaler.save')\n",
        "best_model.save_model('xgboost_model.json')"
      ],
      "metadata": {
        "id": "yOR3qej59nWB",
        "outputId": "be3831d3-93b0-404a-c988-217c317f198e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-718d1c5f3f5a>:24: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df_labeled.iloc[:, df_labeled.columns != 'Label'] = imputer.fit_transform(df_labeled.iloc[:, df_labeled.columns != 'Label'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Enhanced Accuracy: 0.9636363636363636\n",
            "Enhanced F1 Score: 0.9636151738117704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "L7MNUjYq9iHE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(label_encoder, 'label_encoder.save')"
      ],
      "metadata": {
        "id": "In-yjEjx9iED",
        "outputId": "edee538a-5ac2-4217-d046-d6ca87776589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder.save']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scapy.all import rdpcap, IP, TCP\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def extract_features(pcap_file):\n",
        "    packets = rdpcap(pcap_file)\n",
        "    features = {\n",
        "        'Flow Duration': 0.0,\n",
        "        'Total Fwd Packet': 0,\n",
        "        'Total Bwd packets': 0,\n",
        "        'Flow Bytes/s': 0.0,\n",
        "        'Flow Packets/s': 0.0,\n",
        "        'Fwd IAT Mean': 0.0,\n",
        "        'Flow IAT Max': 0.0,\n",
        "        'Flow IAT Min': np.inf,\n",
        "        'Fwd IAT Total': 0.0,\n",
        "        'Packet Length Min': np.inf,\n",
        "        'Packet Length Mean': 0.0,\n",
        "        'ACK Flag Count': 0,\n",
        "        'FWD Init Win Bytes': 0,\n",
        "        'Bwd Init Win Bytes': 0,\n",
        "        'Idle Mean': 0.0,\n",
        "        'Idle Std': 0.0,\n",
        "        'Idle Max': 0.0,\n",
        "        'Idle Min': np.inf,\n",
        "\n",
        "    }\n",
        "    scaler = joblib.load('scaler.save')\n",
        "\n",
        "    if not packets:\n",
        "        return pd.DataFrame(features, index=[0])\n",
        "\n",
        "    start_times = []\n",
        "    packet_lengths = []\n",
        "    iats = []\n",
        "    total_bytes = 0\n",
        "\n",
        "    for packet in packets:\n",
        "        if IP in packet and TCP in packet:\n",
        "            packet_length = len(packet)\n",
        "            packet_lengths.append(packet_length)\n",
        "            total_bytes += packet_length\n",
        "\n",
        "            if 'S' in packet[TCP].flags:\n",
        "                if features['FWD Init Win Bytes'] == 0:\n",
        "                    features['FWD Init Win Bytes'] = packet[TCP].window\n",
        "                else:\n",
        "                    features['Bwd Init Win Bytes'] = packet[TCP].window\n",
        "\n",
        "            if 'A' in packet[TCP].flags:\n",
        "                features['ACK Flag Count'] += 1\n",
        "\n",
        "            start_times.append(float(packet.time))\n",
        "\n",
        "            if len(start_times) > 1:\n",
        "                iat = start_times[-1] - start_times[-2]\n",
        "                iats.append(iat)\n",
        "\n",
        "    features['Flow Duration'] = max(start_times) - min(start_times)\n",
        "    features['Total Fwd Packet'] = len([p for p in packets if IP in p and p[IP].src < p[IP].dst])\n",
        "    features['Total Bwd packets'] = len([p for p in packets if IP in p and p[IP].src > p[IP].dst])\n",
        "    features['Packet Length Min'] = min(packet_lengths)\n",
        "    features['Packet Length Mean'] = np.mean(packet_lengths) if packet_lengths else 0\n",
        "    features['Fwd IAT Total'] = sum(iats)\n",
        "    features['Flow IAT Min'] = min(iats) if iats else 0\n",
        "    features['Flow IAT Max'] = max(iats) if iats else 0\n",
        "    features['Flow IAT Min'] = np.mean(iats) if iats else 0\n",
        "    features['Flow Packets/s'] = len(packets) / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "    features['Flow Bytes/s'] = total_bytes / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "\n",
        "    # Handle potential NaNs and infs\n",
        "    for key, value in features.items():\n",
        "        if isinstance(value, float) and (np.isinf(value) or np.isnan(value)):\n",
        "            features[key] = 0\n",
        "\n",
        "    df_features = pd.DataFrame([features])\n",
        "\n",
        "\n",
        "# Handle potential NaNs and infs again before returning\n",
        "    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_features.fillna(0, inplace=True)\n",
        "    df_features_scaled = scaler.transform(df_features[numerical_features])\n",
        "\n",
        "    return pd.DataFrame(df_features_scaled, columns=numerical_features)"
      ],
      "metadata": {
        "id": "NwrpewCW9iBK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Define the path to your model and pcap file\n",
        "model_path = '/content/xgboost_model.json'  # Make sure this is the correct path to your model file\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/zeronet_00001_20200421125502.pcap'\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "xgb_classifier = xgb.XGBClassifier()\n",
        "xgb_classifier.load_model(model_path)\n",
        "\n",
        "# Extract features from the pcap file\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Prepare the features for the model (make sure df_features is a DataFrame with the correct shape)\n",
        "# The model expects a 2D array-like structure. If df_features is a Series, convert it to a DataFrame\n",
        "\n",
        "# Check the predicted class and print the corresponding label\n",
        "\n",
        "\n",
        "# Output the predicted class\n",
        "#print(\"Predicted Class:\", predictions[0])"
      ],
      "metadata": {
        "id": "GBTZtMb-9h9v"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(df_features, pd.Series):\n",
        "    df_features = df_features.to_frame().transpose()\n",
        "\n",
        "# Predict using the loaded model\n",
        "# Ensure df_features does not have an index column or any non-numeric columns that the model wasn't trained on\n",
        "predictions = xgb_classifier.predict(df_features)"
      ],
      "metadata": {
        "id": "5KAjI8VV9h27"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a binary file\n",
        "best_model.save_model('xgblevel2_model.json')"
      ],
      "metadata": {
        "id": "fstNAYEN9hkO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = xgb.XGBClassifier()\n",
        "loaded_model.load_model('xgblevel2_model.json')"
      ],
      "metadata": {
        "id": "AkgzgCKf9hun"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_encoder.classes_)  # This will show you the mapping\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "id": "n6BZQgls9h0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "model_path = '/content/xgboost_model.json'\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/zeronet_00001_20200506213643.pcap'\n",
        "scaler_path = '/content/scaler.save'\n",
        "label_encoder_path = '/content/label_encoder.save'\n",
        "\n",
        "# Load the trained XGBoost model, scaler, and LabelEncoder\n",
        "xgb_classifier = xgb.XGBClassifier()\n",
        "xgb_classifier.load_model(model_path)\n",
        "scaler = joblib.load(scaler_path)\n",
        "label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "# Extract features from the pcap file\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Scale the extracted features\n",
        "df_features_scaled = scaler.transform(df_features)\n",
        "\n",
        "# Predict using the scaled features\n",
        "predictions_encodeds = xgb_classifier.predict(df_features_scaled)\n",
        "\n",
        "# Decode the predictions to original class labels\n",
        "predict = label_encoder.inverse_transform(predictions_encodeds)\n",
        "\n",
        "# Output the prediction\n",
        "print(label_encoder.classes_)  # This shows the mapping\n",
        "print(predict[0])  # This shows the predicted class"
      ],
      "metadata": {
        "id": "BBi5dkdx-Grx",
        "outputId": "8fab1b86-7252-4c3d-db63-2dd7be036a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['FreeNet' 'I2P' 'Tor' 'ZeroNet']\n",
            "ZeroNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 RD LEVEL CLASSIFIER-ZERONET**"
      ],
      "metadata": {
        "id": "T8EvF-gKAzmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data from CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv/zeronet-behavior.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Fill NaN values with zeros\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Define the features to keep\n",
        "features_to_keep = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Packet Length Min', 'Packet Length Mean', 'Fwd IAT Total',\n",
        "    'Flow IAT Min', 'Flow IAT Max', 'Fwd IAT Mean', 'Flow Packets/s',\n",
        "    'Flow Bytes/s', 'Idle Min', 'Idle Max', 'Idle Mean',\n",
        "    'Idle Std', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'ACK Flag Count'\n",
        "]\n",
        "\n",
        "# Keep only the relevant features\n",
        "df_relevant_features = data[features_to_keep + ['Label']]\n",
        "\n",
        "# Convert label column to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df_relevant_features.drop('Label', axis=1)\n",
        "y = df_relevant_features['Label']\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "categorical_columns = X.select_dtypes(include='object').columns\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating LightGBM dataset\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test)\n",
        "\n",
        "# Setting parameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(data['Label'].unique()),\n",
        "    'metric': 'multi_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Training the model\n",
        "num_round = 100\n",
        "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
        "\n",
        "# Making predictions\n",
        "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to predicted classes\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "iezm28kwdaU_",
        "outputId": "7b28dfc2-cc2a-437b-a0a5-8349dd387838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-00239b23561b>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "Accuracy: 0.9205426356589147\n",
            "F1 Score: 0.9204826210689786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bst.save_model('trained_lightgbm_model.txt')"
      ],
      "metadata": {
        "id": "eMTsqfzXZ3Yh",
        "outputId": "2433cef8-d621-4790-de62-d38b9acd6efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7a98a3458f40>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = lgb.Booster(model_file='trained_lightgbm_model.txt')\n"
      ],
      "metadata": {
        "id": "HpCV7JHvcS1Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "def classify_trafficlgb(df_features, model_path):\n",
        "    # Load the trained model\n",
        "    lgb_model = lgb.Booster(model_file=model_path)\n",
        "\n",
        "    # Predict the traffic class\n",
        "    predictions = lgb_model.predict(df_features)\n",
        "\n",
        "    # Convert probability predictions to class labels\n",
        "    predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "    return predicted_classes\n"
      ],
      "metadata": {
        "id": "JJT9WIGbcaSm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Define the path to your model and pcap file\n",
        "model_path = '/content/trained_lightgbm_model.txt'  # Make sure this is the correct path to your model file\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/zeronet_00001_20200421125502.pcap'\n",
        "\n",
        "# Load the trained LightGBM model\n",
        "lgb_model = lgb.Booster(model_file=model_path)\n",
        "\n",
        "# Extract features from the pcap file\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Prepare the features for the model (make sure df_features is a DataFrame with the correct shape)\n",
        "# The model expects a 2D array-like structure. If df_features is a Series, convert it to a DataFrame\n",
        "\n",
        "# Check the predicted class and print the corresponding label\n",
        "predictionz = lgb_model.predict(df_features)\n",
        "predicted_class = label_encoder.inverse_transform(predictionz.argmax(axis=1))[0]\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "TNvP_WFsZ0PB",
        "outputId": "a55a351e-8237-49a9-dac9-e0bce214ba86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: browsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOR**"
      ],
      "metadata": {
        "id": "qmWgXk-PgKaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load data from CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv/tor-behavior.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Identify and encode categorical variables\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    data[col] = label_encoders[col].fit_transform(data[col])\n",
        "\n",
        "# Separate features and target variable\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Fill NaN values with zeros\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Define the features to keep\n",
        "features_to_keep = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Packet Length Min', 'Packet Length Mean', 'Fwd IAT Total',\n",
        "    'Flow IAT Min', 'Flow IAT Max', 'Fwd IAT Mean', 'Flow Packets/s',\n",
        "    'Flow Bytes/s', 'Idle Min', 'Idle Max', 'Idle Mean',\n",
        "    'Idle Std', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'ACK Flag Count'\n",
        "]\n",
        "\n",
        "# Keep only the relevant features\n",
        "df_relevant_features = data[features_to_keep + ['Label']]\n",
        "\n",
        "X = df_relevant_features.drop('Label', axis=1)\n",
        "y = df_relevant_features['Label']\n",
        "\n",
        "# Convert label column to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n",
        "\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize numerical features\n",
        "numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "scaler = StandardScaler()\n",
        "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
        "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n",
        "\n",
        "# Convert data to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Setting parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # Assuming you have multiple classes\n",
        "    'num_class': len(data['Label'].unique()),  # Number of unique classes\n",
        "    'eval_metric': 'mlogloss',  # you can use different metrics here\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "# Training the model\n",
        "num_round = 100\n",
        "bst = xgb.train(params, dtrain, num_round, evals=[(dtest, 'eval')])\n",
        "\n",
        "# Making predictions\n",
        "y_predt = bst.predict(dtest)\n"
      ],
      "metadata": {
        "id": "Hx9wkLM2fcYa",
        "outputId": "90dee1bd-3151-48ff-935d-c47bdfa44381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-d2b3611ec250>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\teval-mlogloss:1.97284\n",
            "[1]\teval-mlogloss:1.88553\n",
            "[2]\teval-mlogloss:1.80949\n",
            "[3]\teval-mlogloss:1.74308\n",
            "[4]\teval-mlogloss:1.68023\n",
            "[5]\teval-mlogloss:1.62412\n",
            "[6]\teval-mlogloss:1.57255\n",
            "[7]\teval-mlogloss:1.52437\n",
            "[8]\teval-mlogloss:1.48040\n",
            "[9]\teval-mlogloss:1.43919\n",
            "[10]\teval-mlogloss:1.40200\n",
            "[11]\teval-mlogloss:1.36810\n",
            "[12]\teval-mlogloss:1.33607\n",
            "[13]\teval-mlogloss:1.30549\n",
            "[14]\teval-mlogloss:1.27656\n",
            "[15]\teval-mlogloss:1.24989\n",
            "[16]\teval-mlogloss:1.22560\n",
            "[17]\teval-mlogloss:1.20259\n",
            "[18]\teval-mlogloss:1.17969\n",
            "[19]\teval-mlogloss:1.15965\n",
            "[20]\teval-mlogloss:1.13993\n",
            "[21]\teval-mlogloss:1.12096\n",
            "[22]\teval-mlogloss:1.10286\n",
            "[23]\teval-mlogloss:1.08573\n",
            "[24]\teval-mlogloss:1.06981\n",
            "[25]\teval-mlogloss:1.05431\n",
            "[26]\teval-mlogloss:1.03964\n",
            "[27]\teval-mlogloss:1.02540\n",
            "[28]\teval-mlogloss:1.01192\n",
            "[29]\teval-mlogloss:0.99877\n",
            "[30]\teval-mlogloss:0.98691\n",
            "[31]\teval-mlogloss:0.97560\n",
            "[32]\teval-mlogloss:0.96429\n",
            "[33]\teval-mlogloss:0.95413\n",
            "[34]\teval-mlogloss:0.94365\n",
            "[35]\teval-mlogloss:0.93419\n",
            "[36]\teval-mlogloss:0.92444\n",
            "[37]\teval-mlogloss:0.91549\n",
            "[38]\teval-mlogloss:0.90619\n",
            "[39]\teval-mlogloss:0.89828\n",
            "[40]\teval-mlogloss:0.89013\n",
            "[41]\teval-mlogloss:0.88268\n",
            "[42]\teval-mlogloss:0.87530\n",
            "[43]\teval-mlogloss:0.86855\n",
            "[44]\teval-mlogloss:0.86152\n",
            "[45]\teval-mlogloss:0.85558\n",
            "[46]\teval-mlogloss:0.84990\n",
            "[47]\teval-mlogloss:0.84377\n",
            "[48]\teval-mlogloss:0.83828\n",
            "[49]\teval-mlogloss:0.83283\n",
            "[50]\teval-mlogloss:0.82808\n",
            "[51]\teval-mlogloss:0.82320\n",
            "[52]\teval-mlogloss:0.81805\n",
            "[53]\teval-mlogloss:0.81385\n",
            "[54]\teval-mlogloss:0.80880\n",
            "[55]\teval-mlogloss:0.80451\n",
            "[56]\teval-mlogloss:0.80080\n",
            "[57]\teval-mlogloss:0.79680\n",
            "[58]\teval-mlogloss:0.79286\n",
            "[59]\teval-mlogloss:0.78886\n",
            "[60]\teval-mlogloss:0.78504\n",
            "[61]\teval-mlogloss:0.78170\n",
            "[62]\teval-mlogloss:0.77778\n",
            "[63]\teval-mlogloss:0.77487\n",
            "[64]\teval-mlogloss:0.77126\n",
            "[65]\teval-mlogloss:0.76841\n",
            "[66]\teval-mlogloss:0.76516\n",
            "[67]\teval-mlogloss:0.76236\n",
            "[68]\teval-mlogloss:0.75964\n",
            "[69]\teval-mlogloss:0.75670\n",
            "[70]\teval-mlogloss:0.75413\n",
            "[71]\teval-mlogloss:0.75149\n",
            "[72]\teval-mlogloss:0.74895\n",
            "[73]\teval-mlogloss:0.74674\n",
            "[74]\teval-mlogloss:0.74431\n",
            "[75]\teval-mlogloss:0.74135\n",
            "[76]\teval-mlogloss:0.73923\n",
            "[77]\teval-mlogloss:0.73683\n",
            "[78]\teval-mlogloss:0.73430\n",
            "[79]\teval-mlogloss:0.73192\n",
            "[80]\teval-mlogloss:0.73051\n",
            "[81]\teval-mlogloss:0.72870\n",
            "[82]\teval-mlogloss:0.72659\n",
            "[83]\teval-mlogloss:0.72457\n",
            "[84]\teval-mlogloss:0.72307\n",
            "[85]\teval-mlogloss:0.72157\n",
            "[86]\teval-mlogloss:0.71957\n",
            "[87]\teval-mlogloss:0.71777\n",
            "[88]\teval-mlogloss:0.71645\n",
            "[89]\teval-mlogloss:0.71473\n",
            "[90]\teval-mlogloss:0.71344\n",
            "[91]\teval-mlogloss:0.71230\n",
            "[92]\teval-mlogloss:0.71085\n",
            "[93]\teval-mlogloss:0.70947\n",
            "[94]\teval-mlogloss:0.70839\n",
            "[95]\teval-mlogloss:0.70725\n",
            "[96]\teval-mlogloss:0.70630\n",
            "[97]\teval-mlogloss:0.70496\n",
            "[98]\teval-mlogloss:0.70364\n",
            "[99]\teval-mlogloss:0.70267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a binary file\n",
        "bst.save_model('xgblevel3t_model.json')\n",
        "\n"
      ],
      "metadata": {
        "id": "6-cvE8clizoq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = xgb.XGBClassifier()\n",
        "loaded_model.load_model('xgblevel3t_model.json')"
      ],
      "metadata": {
        "id": "71Om-o2zip6u"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "def classify_traffict(df_features, model_path):\n",
        "    # Load the trained model\n",
        "    xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "    # Load the model\n",
        "    xgb_model.load_model(model_path)\n",
        "\n",
        "    # Ensure that the model has been fitted before making predictions\n",
        "    if not xgb_model.get_booster().attr(\"n_features\"):\n",
        "        raise ValueError(\"Model needs to be fitted before making predictions\")\n",
        "\n",
        "    # Predict the traffic class\n",
        "    predictions = xgb_model.predict(df_features)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "8ZbrR2xmjp-m"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Define the path to your model and pcap file\n",
        "model_path = '/content/xgblevel3t_model.json'  # Make sure this is the correct path to your model file\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/tor_00001_20200417155424.pcap'\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "bst = xgb.XGBClassifier()\n",
        "bst.load_model(model_path)\n",
        "\n",
        "# Extract features from the pcap file\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Prepare the features for the model (make sure df_features is a DataFrame with the correct shape)\n",
        "# The model expects a 2D array-like structure. If df_features is a Series, convert it to a DataFrame\n",
        "\n",
        "# Check the predicted class and print the corresponding label\n",
        "\n",
        "\n",
        "# Output the predicted class\n",
        "#print(\"Predicted Class:\", predictions[0])\n"
      ],
      "metadata": {
        "id": "zvDWt87Uj3Xc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(df_features, pd.Series):\n",
        "    df_features = df_features.to_frame().transpose()\n",
        "\n",
        "# Predict using the loaded model\n",
        "# Ensure df_features does not have an index column or any non-numeric columns that the model wasn't trained on\n",
        "predictiont = bst.predict(df_features)\n",
        "predicted_class = label_encoder.inverse_transform(predictionz.argmax(axis=1))[0]\n",
        "print(\"Predicted Class:\", predicted_class)\n",
        "\n"
      ],
      "metadata": {
        "id": "NEs9Sshbke3A",
        "outputId": "68868d99-d650-4bc8-a4b4-e00989104a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  FREENET"
      ],
      "metadata": {
        "id": "npcDpVZtIJrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load data from CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv/freenet-behavior.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Fill NaN values with zeros\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Define the features to keep\n",
        "features_to_keep = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Packet Length Min', 'Packet Length Mean', 'Fwd IAT Total',\n",
        "    'Flow IAT Min', 'Flow IAT Max', 'Fwd IAT Mean', 'Flow Packets/s',\n",
        "    'Flow Bytes/s', 'Idle Min', 'Idle Max', 'Idle Mean',\n",
        "    'Idle Std', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'ACK Flag Count'\n",
        "]\n",
        "\n",
        "# Keep only the relevant features\n",
        "df_relevant_features = data[features_to_keep + ['Label']]\n",
        "\n",
        "# Convert label column to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df_relevant_features.drop('Label', axis=1)\n",
        "y = df_relevant_features['Label']\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "categorical_columns = X.select_dtypes(include='object').columns\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Setting parameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(df_relevant_features['Label'].unique()),\n",
        "    'metric': 'multi_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Training the model\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "bst = lgb.train(params, train_data)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = bst.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to predicted classes\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "1SlyCQBRIMDd",
        "outputId": "58de60c4-5c75-49e8-df83-4120dd6d680c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-ef16c09078ee>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Accuracy: 0.963392312385601\n",
            "F1 Score: 0.9634453142252268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Assume 'bst' is your trained LightGBM model\n",
        "\n",
        "# Save the model\n",
        "bst.save_model('gbdt_model.txt')\n"
      ],
      "metadata": {
        "id": "SJ-vH7PHItFd",
        "outputId": "6a9ca8d4-d4ff-4d1e-c0de-4c00875009fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7a982c3d5c30>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = lgb.Booster(model_file='gbdt_model.txt')\n"
      ],
      "metadata": {
        "id": "MuNLyEQTIyzL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def classify_traffic_gbdt(df_features, model_path):\n",
        "    \"\"\"\n",
        "    Classifies traffic using a GBDT model.\n",
        "\n",
        "    Parameters:\n",
        "    - df_features (2D array-like): Features of the traffic data.\n",
        "    - model_path (str): Path to the trained GBDT model file.\n",
        "\n",
        "    Returns:\n",
        "    - predicted_classes (array-like): Predicted classes for the traffic data.\n",
        "    \"\"\"\n",
        "    # Load the trained GBDT model\n",
        "    model = loaded_model(model_path)\n",
        "\n",
        "    # Predict the traffic class probabilities\n",
        "    predictions = model.predict(df_features)\n",
        "\n",
        "    # Convert probability predictions to class labels\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return predicted_classes\n"
      ],
      "metadata": {
        "id": "jpxIlgPFI4KM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Define the path to your model and pcap file\n",
        "model_path = '/content/gbdt_model.txt'  # Make sure this is the correct path to your model file\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/freenet-1_00001_20200416195121.pcap'\n",
        "\n",
        "# Load the trained LightGBM model\n",
        "lgb_model = lgb.Booster(model_file=model_path)\n",
        "\n",
        "# Extract features from the pcap file (make sure you have a function named 'extract_features' for this)\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Prepare the features for the model (make sure df_features is a DataFrame with the correct shape)\n",
        "# The model expects a 2D array-like structure. If df_features is a Series, convert it to a DataFrame\n",
        "\n",
        "# Check the predicted class and print the corresponding label\n",
        "predictions = lgb_model.predict(df_features)\n",
        "predicted_class = predictions.argmax(axis=1)[0]  # Assuming you're predicting only one instance\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "GhUJWdR9I91Y",
        "outputId": "7e726371-8055-4fb3-87ca-8fce24b451ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = label_encoder.inverse_transform(predictions.argmax(axis=1))[0]"
      ],
      "metadata": {
        "id": "eWGCnqBeL-tN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "_d-pHp6GMEOw",
        "outputId": "29922334-b0cf-450d-c4db-4d7ae452e2ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: ftp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I2P"
      ],
      "metadata": {
        "id": "wKxr-thTJifK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load data from CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/csv/i2p-behavior.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Fill NaN values with zeros\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Define the features to keep\n",
        "features_to_keep = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Packet Length Min', 'Packet Length Mean', 'Fwd IAT Total',\n",
        "    'Flow IAT Min', 'Flow IAT Max', 'Fwd IAT Mean', 'Flow Packets/s',\n",
        "    'Flow Bytes/s', 'Idle Min', 'Idle Max', 'Idle Mean',\n",
        "    'Idle Std', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'ACK Flag Count'\n",
        "]\n",
        "\n",
        "# Keep only the relevant features\n",
        "df_relevant_features = data[features_to_keep + ['Label']]\n",
        "\n",
        "# Convert label column to numerical values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n",
        "\n",
        "# Split the data into features and labels\n",
        "X = df_relevant_features.drop('Label', axis=1)\n",
        "y = df_relevant_features['Label']\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "categorical_columns = X.select_dtypes(include='object').columns\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': len(df_relevant_features['Label'].unique()),\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "# Convert data to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Training the model\n",
        "num_round = 100\n",
        "bstI2P = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = bstI2P.predict(dtest)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "STH3V5qpJe4-",
        "outputId": "769b5e0c-59fe-4c75-bf65-d73491b2ae1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-cd80b924d9a2>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_relevant_features['Label'] = label_encoder.fit_transform(df_relevant_features['Label'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9226993865030675\n",
            "F1 Score: 0.9226692075187037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Define the path where you want to save the model\n",
        "model_path = 'xgboost_model.model'\n",
        "\n",
        "# Train your XGBoost model (assuming 'bst' is your trained XGBoost model)\n",
        "# Example:\n",
        "# bst = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Save the trained model\n",
        "bstI2P.save_model(model_path)\n",
        "\n",
        "print(\"XGBoost model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "xY_H7dMwKQtB",
        "outputId": "44e0356e-a0ca-4e7b-a9ad-2642978fc876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Define the path where the model is saved\n",
        "model_path = 'xgboost_model.model'\n",
        "\n",
        "# Load the saved XGBoost model\n",
        "bstI2P = xgb.Booster()\n",
        "bstI2P.load_model(model_path)\n",
        "\n",
        "print(\"XGBoost model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "ukuKZxL6KV7Z",
        "outputId": "2549afde-44d5-43ea-d125-8f71fa1d8192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "def classify_traffic_xgb(df_features, model_path):\n",
        "    \"\"\"\n",
        "    Classifies traffic using a trained XGBoost model.\n",
        "\n",
        "    Parameters:\n",
        "    - df_features (DataFrame): DataFrame containing features of the traffic data.\n",
        "    - model_path (str): Path to the trained XGBoost model file.\n",
        "\n",
        "    Returns:\n",
        "    - predicted_classes (array-like): Predicted classes for the traffic data.\n",
        "    \"\"\"\n",
        "    # Load the trained XGBoost model\n",
        "    xgb_model = xgb.Booster()\n",
        "    xgb_model.load_model(model_path)\n",
        "\n",
        "    # Convert features to DMatrix format\n",
        "    dmatrix = xgb.DMatrix(df_features)\n",
        "\n",
        "    # Predict the traffic class\n",
        "    predictions = xgb_model.predict(dmatrix)\n",
        "\n",
        "    # Convert probability predictions to class labels\n",
        "    predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "    return predicted_classes\n"
      ],
      "metadata": {
        "id": "EYTXII3-KumX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Define the path to your model and pcap file\n",
        "model_path = '/content/xgboost_model.model'  # Make sure this is the correct path to your model file\n",
        "pcap_file_path = '/content/drive/MyDrive/Colab Notebooks/pcap/i2p_00001_20200407201731.pcap'\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "xgb_model = xgb.Booster()\n",
        "xgb_model.load_model(model_path)\n",
        "\n",
        "# Extract features from the pcap file\n",
        "df_features = extract_features(pcap_file_path)\n",
        "\n",
        "# Prepare the features for the model (make sure df_features is a DataFrame with the correct shape)\n",
        "# The model expects a 2D array-like structure. If df_features is a Series, convert it to a DataFrame\n",
        "\n",
        "# Check the predicted class and print the corresponding label\n",
        "predictions = xgb_model.predict(xgb.DMatrix(df_features))\n",
        "predicted_class = predictions.argmax()  # No need for axis=1 as predictions is a 1D array\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "tbhn07D0KyZB",
        "outputId": "2614557f-dcc9-4bc5-adbe-90dde4ea156d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = label_encoder.inverse_transform(predictionz.argmax(axis=1))[0]\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "M7HfxgPHMbBl",
        "outputId": "37dba5c8-58d5-44f1-b0dd-4ded32167ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Flask**"
      ],
      "metadata": {
        "id": "uez-GdA5JmRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask\n",
        "!pip install scapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZGQsGsULAgI",
        "outputId": "54fc7d4a-82f0-462f-e8d4-92afc990b4bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: scapy in /usr/local/lib/python3.10/dist-packages (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-ngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJGtJ7SwA4eB",
        "outputId": "f8e2e14b-3865-4de4-935f-8342ee9c59e0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgXYpgkdLAWo",
        "outputId": "725065a9-57aa-4aa1-efb8-af9d48f8c433"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWJUBD43LAa5",
        "outputId": "47b1ac6e-476b-4ab7-8bf4-5e4534bb5169"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken '2cmVpIezM4pvk5PwnsBcrtZV5q3_6C4eAXASSwyqqrJH1qDqw'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TziZowv1LANU",
        "outputId": "2e0201f0-35ba-46c7-c145-568f0094f234"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask-cors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxD4MASERl75",
        "outputId": "de9974ae-be28-42ae-c933-a8a74dae4551"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.5)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Setup a tunnel to the Flask application on port 5000\n",
        "ngrok_tunnel = ngrok.connect(5000)\n"
      ],
      "metadata": {
        "id": "0-GuSerlSRv2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask_cors import CORS  # Ensure Flask-CORS is installed\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scapy.all import rdpcap, IP, TCP\n",
        "import xgboost as xgb  # Ensure XGBoost is installed\n",
        "\n",
        "app = Flask(__name__, template_folder='.')\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
        "# Automatically create the uploads directory if it doesn't exist\n",
        "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "CORS(app)  # Enable CORS for all domains on all routes\n",
        "\n",
        "# Load the XGBoost model (adjust the path to where your model is located)\n",
        "MODEL_PATH = '/content/xgb_model.json'  # Update this path\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.load_model(MODEL_PATH)\n",
        "\n",
        "def preprocess_pcap(file_path):\n",
        "    packets = rdpcap(file_path)\n",
        "    features = {\n",
        "        'Flow Duration': 0.0,\n",
        "        'Total Fwd Packet': 0,\n",
        "        'Total Bwd packets': 0,\n",
        "        'Packet Length Min': np.inf,\n",
        "        'Packet Length Mean': 0.0,\n",
        "        'Fwd IAT Total': 0.0,\n",
        "        'Flow IAT Min': np.inf,\n",
        "        'Flow IAT Max': 0.0,\n",
        "        'Fwd IAT Mean': 0.0,\n",
        "        'Flow Packets/s': 0.0,\n",
        "        'Flow Bytes/s': 0.0,\n",
        "        'Idle Min': np.inf,\n",
        "        'Idle Max': 0.0,\n",
        "        'Idle Mean': 0.0,\n",
        "        'Idle Std': 0.0,\n",
        "        'FWD Init Win Bytes': 0,\n",
        "        'Bwd Init Win Bytes': 0,\n",
        "        'ACK Flag Count': 0\n",
        "    }\n",
        "\n",
        "    if not packets:\n",
        "        return pd.DataFrame(features, index=[0])\n",
        "\n",
        "    start_times = []\n",
        "    packet_lengths = []\n",
        "    iats = []\n",
        "    total_bytes = 0\n",
        "\n",
        "    for packet in packets:\n",
        "        if IP in packet and TCP in packet:\n",
        "            packet_length = len(packet)\n",
        "            packet_lengths.append(packet_length)\n",
        "            total_bytes += packet_length\n",
        "\n",
        "            if 'S' in packet[TCP].flags:\n",
        "                if features['FWD Init Win Bytes'] == 0:\n",
        "                    features['FWD Init Win Bytes'] = packet[TCP].window\n",
        "                else:\n",
        "                    features['Bwd Init Win Bytes'] = packet[TCP].window\n",
        "\n",
        "            if 'A' in packet[TCP].flags:\n",
        "                features['ACK Flag Count'] += 1\n",
        "\n",
        "            start_times.append(float(packet.time))\n",
        "\n",
        "            if len(start_times) > 1:\n",
        "                iat = start_times[-1] - start_times[-2]\n",
        "                iats.append(iat)\n",
        "\n",
        "    features['Flow Duration'] = max(start_times) - min(start_times)\n",
        "    features['Total Fwd Packet'] = len([p for p in packets if IP in p and p[IP].src < p[IP].dst])\n",
        "    features['Total Bwd packets'] = len([p for p in packets if IP in p and p[IP].src > p[IP].dst])\n",
        "    features['Packet Length Min'] = min(packet_lengths)\n",
        "    features['Packet Length Mean'] = np.mean(packet_lengths) if packet_lengths else 0\n",
        "    features['Fwd IAT Total'] = sum(iats)\n",
        "    features['Flow IAT Min'] = min(iats) if iats else 0\n",
        "    features['Flow IAT Max'] = max(iats) if iats else 0\n",
        "    features['Fwd IAT Mean'] = np.mean(iats) if iats else 0\n",
        "    features['Flow Packets/s'] = len(packets) / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "    features['Flow Bytes/s'] = total_bytes / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_features = pd.DataFrame([features])\n",
        "    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_features.fillna(0, inplace=True)\n",
        "    return df_features\n",
        "\n",
        "# Prediction function using the preprocessed data\n",
        "def predict_darknet(file_path):\n",
        "    df_features = preprocess_pcap(file_path)\n",
        "    # Assuming the model expects a DMatrix\n",
        "    dmatrix = xgb.DMatrix(df_features)\n",
        "    predictions = xgb_model.predict(dmatrix)\n",
        "    return predictions\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'}), 400\n",
        "\n",
        "    try:\n",
        "        filename = secure_filename(file.filename)\n",
        "        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        predictions = predict_darknet(file_path)\n",
        "        return jsonify({'prediction': str(predictions)})\n",
        "    except Exception as e:\n",
        "        # Log the exception to your Flask app log, if needed\n",
        "        return jsonify({'error': 'Failed to process the file. Please try again.'}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRvc7UpkA4bg",
        "outputId": "b221db35-8d6a-485f-a155-fc5317fe006e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, render_template\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask_cors import CORS  # Ensure Flask-CORS is installed\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scapy.all import rdpcap, IP, TCP\n",
        "import xgboost as xgb  # Ensure XGBoost is installed\n",
        "\n",
        "app = Flask(__name__, template_folder='.')\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
        "# Automatically create the uploads directory if it doesn't exist\n",
        "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "CORS(app)  # Enable CORS for all domains on all routes\n",
        "\n",
        "# Load the XGBoost model (adjust the path to where your model is located)\n",
        "MODEL_PATH = '/content/xgb_model.json'  # Update this path\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.load_model(MODEL_PATH)\n",
        "\n",
        "def preprocess_pcap(file_path):\n",
        "    packets = rdpcap(file_path)\n",
        "    features = {\n",
        "        'Flow Duration': 0.0,\n",
        "        'Total Fwd Packet': 0,\n",
        "        'Total Bwd packets': 0,\n",
        "        'Packet Length Min': np.inf,\n",
        "        'Packet Length Mean': 0.0,\n",
        "        'Fwd IAT Total': 0.0,\n",
        "        'Flow IAT Min': np.inf,\n",
        "        'Flow IAT Max': 0.0,\n",
        "        'Fwd IAT Mean': 0.0,\n",
        "        'Flow Packets/s': 0.0,\n",
        "        'Flow Bytes/s': 0.0,\n",
        "        'Idle Min': np.inf,\n",
        "        'Idle Max': 0.0,\n",
        "        'Idle Mean': 0.0,\n",
        "        'Idle Std': 0.0,\n",
        "        'FWD Init Win Bytes': 0,\n",
        "        'Bwd Init Win Bytes': 0,\n",
        "        'ACK Flag Count': 0\n",
        "    }\n",
        "\n",
        "    if not packets:\n",
        "        return pd.DataFrame(features, index=[0])\n",
        "\n",
        "    start_times = []\n",
        "    packet_lengths = []\n",
        "    iats = []\n",
        "    total_bytes = 0\n",
        "\n",
        "    for packet in packets:\n",
        "        if IP in packet and TCP in packet:\n",
        "            packet_length = len(packet)\n",
        "            packet_lengths.append(packet_length)\n",
        "            total_bytes += packet_length\n",
        "\n",
        "            if 'S' in packet[TCP].flags:\n",
        "                if features['FWD Init Win Bytes'] == 0:\n",
        "                    features['FWD Init Win Bytes'] = packet[TCP].window\n",
        "                else:\n",
        "                    features['Bwd Init Win Bytes'] = packet[TCP].window\n",
        "\n",
        "            if 'A' in packet[TCP].flags:\n",
        "                features['ACK Flag Count'] += 1\n",
        "\n",
        "            start_times.append(float(packet.time))\n",
        "\n",
        "            if len(start_times) > 1:\n",
        "                iat = start_times[-1] - start_times[-2]\n",
        "                iats.append(iat)\n",
        "\n",
        "    features['Flow Duration'] = max(start_times) - min(start_times)\n",
        "    features['Total Fwd Packet'] = len([p for p in packets if IP in p and p[IP].src < p[IP].dst])\n",
        "    features['Total Bwd packets'] = len([p for p in packets if IP in p and p[IP].src > p[IP].dst])\n",
        "    features['Packet Length Min'] = min(packet_lengths)\n",
        "    features['Packet Length Mean'] = np.mean(packet_lengths) if packet_lengths else 0\n",
        "    features['Fwd IAT Total'] = sum(iats)\n",
        "    features['Flow IAT Min'] = min(iats) if iats else 0\n",
        "    features['Flow IAT Max'] = max(iats) if iats else 0\n",
        "    features['Fwd IAT Mean'] = np.mean(iats) if iats else 0\n",
        "    features['Flow Packets/s'] = len(packets) / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "    features['Flow Bytes/s'] = total_bytes / features['Flow Duration'] if features['Flow Duration'] else 0\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_features = pd.DataFrame([features])\n",
        "    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_features.fillna(0, inplace=True)\n",
        "    return df_features\n",
        "\n",
        "# Prediction function using the preprocessed data\n",
        "def predict_darknet(file_path):\n",
        "    df_features = preprocess_pcap(file_path)\n",
        "    # Assuming the model expects a DMatrix\n",
        "    dmatrix = xgb.DMatrix(df_features)\n",
        "    predictions = xgb_model.predict(dmatrix)\n",
        "    return predictions\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file part'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No selected file'}), 400\n",
        "\n",
        "    try:\n",
        "        filename = secure_filename(file.filename)\n",
        "        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        predictions = predict_darknet(file_path)\n",
        "        return jsonify({'prediction': str(predictions)})\n",
        "    except Exception as e:\n",
        "        # Log the exception to your Flask app log, if needed\n",
        "        return jsonify({'error': 'Failed to process the file. Please try again.'}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8-64Y2KYOtl",
        "outputId": "3374b57c-4f2b-4b49-8027-af99d5a175c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Z58FnReA4VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSHjuNJACCEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zTLsDaSCCBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ws0qiFUCB-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0_vnXYhCB7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Muy8S63wCB5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88JRl4aPA4SA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}